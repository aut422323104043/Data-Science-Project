# -*- coding: utf-8 -*-
"""source_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12JXh0EaH400T9Sppwnf-vkyAidKN-tKj
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = "student-dataset.csv"  # Update the correct path if needed
df = pd.read_csv(file_path, sep=';')

# Set the style for the plots
sns.set_style("whitegrid")

# Visualize the distribution of students' final grades (G3)
plt.figure(figsize=(8, 5))
sns.histplot(df["G3"], bins=10, kde=True, color="blue")
plt.title("Distribution of Final Grades (G3)")
plt.xlabel("Grade")
plt.ylabel("Frequency")
plt.show()

# Bar plot of student gender distribution
plt.figure(figsize=(6, 4))
sns.countplot(x="sex", data=df, palette="coolwarm")
plt.title("Gender Distribution")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.show()

# Box plot for study time vs final grade
plt.figure(figsize=(8, 5))
sns.boxplot(x="studytime", y="G3", data=df, palette="magma")
plt.title("Study Time vs Final Grade")
plt.xlabel("Study Time (hours/week)")
plt.ylabel("Final Grade (G3)")
plt.show()

# Heatmap showing correlation between numeric variables
# Select only numeric features for correlation calculation
numeric_df = df.select_dtypes(include=['number'])

plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

# Define target variable
target = "G3"  # Final grade
print(f"Target variable: {target}")

# Define feature variables (excluding target)
features = df.columns.tolist()
features.remove(target)
print(f"Feature variables: {features}")

from sklearn.model_selection import train_test_split

# Create feature matrix (X) and target variable (y)
X = df[features]
y = df[target]

# Split dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data split into training and testing sets successfully!")

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Load the dataset
file_path = "student-dataset.csv"  # Update the correct path if needed
df = pd.read_csv(file_path, sep=';')

# Identify categorical columns
categorical_columns = ['sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian']

# Convert categorical columns using Label Encoding
label_encoder = LabelEncoder()
for col in categorical_columns:
    df[col] = label_encoder.fit_transform(df[col])

print("Categorical columns converted to numerical using Label Encoding:")
print(df.head())

# Alternative: One-Hot Encoding
df_one_hot = pd.get_dummies(df, columns=categorical_columns, drop_first=True)
print("\nDataset with One-Hot Encoding applied:")
print(df_one_hot.head())

import pandas as pd

# Load the dataset
file_path = "student-dataset.csv"  # Update with the correct path
df = pd.read_csv(file_path, sep=';')  # Using semicolon as separator

# Identify categorical columns
categorical_columns = ['sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian']

# Apply One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)

print("Dataset after One-Hot Encoding:")
print(df_encoded.head())

from sklearn.preprocessing import StandardScaler

# Load the dataset
file_path = "student-dataset.csv"  # Update the correct path if needed
df = pd.read_csv(file_path, sep=';')

# Select numeric columns for scaling
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Apply Standardization (Z-score normalization)
scaler = StandardScaler()
df_standardized = df.copy()
df_standardized[numeric_columns] = scaler.fit_transform(df[numeric_columns])

print("Data after Standardization (Z-score normalization):")
print(df_standardized.head())

from sklearn.preprocessing import MinMaxScaler

# Apply Min-Max Scaling
scaler = MinMaxScaler()
df_minmax = df.copy()
df_minmax[numeric_columns] = scaler.fit_transform(df[numeric_columns])

print("\nData after Min-Max Scaling:")
print(df_minmax.head())

from sklearn.preprocessing import MinMaxScaler

# Apply Min-Max Scaling
scaler = MinMaxScaler()
df_minmax = df.copy()
df_minmax[numeric_columns] = scaler.fit_transform(df[numeric_columns])

print("\nData after Min-Max Scaling:")
print(df_minmax.head())

# Load dataset
file_path = "student-dataset.csv"  # Ensure correct path
df = pd.read_csv(file_path, sep=';')

# Define features and target
target = "G3"
features = df.columns.tolist()
features.remove(target)

X = df[features]
y = df[target]

from sklearn.preprocessing import OneHotEncoder, StandardScaler
import pandas as pd

# Load dataset
file_path = "student-dataset.csv"  # Ensure correct path
df = pd.read_csv(file_path, sep=';')

# Define features and target
target = "G3"
features = df.columns.tolist()
features.remove(target)

X = df[features]
y = df[target]

# Apply One-Hot Encoding to categorical columns
categorical_columns = ['sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian']
X = pd.get_dummies(X, columns=categorical_columns, drop_first=True)

# Select numerical features for scaling
numeric_features = X.select_dtypes(include=['number']).columns

# Standardize numerical features only
scaler = StandardScaler()
X_scaled = X.copy()  # Create a copy to avoid modifying the original DataFrame
X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features]) # Apply scaling to numerical features only

# Check if 'school' and 'age' columns are causing issues
# If they are not meant to be numerical, you should remove them or one-hot encode them
# For example, if you want to keep only numerical features for scaling:
# numeric_features = [col for col in numeric_features if col not in ['school', 'age']]

# Split into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("Data split into training and testing sets!")

from sklearn.preprocessing import OneHotEncoder, StandardScaler
import pandas as pd

# Load dataset
file_path = "student-dataset.csv"  # Ensure correct path
df = pd.read_csv(file_path, sep=';')

# Define features and target
target = "G3"

# Handle missing values
df.fillna(df.mean(numeric_only=True), inplace=True)  # Fill numerical missing values with mean
df.fillna(df.mode().iloc[0], inplace=True)  # Fill categorical missing values with mode

print("Missing values handled successfully!")

from scipy.stats import zscore

# Remove outliers
df_numeric = df.select_dtypes(include=['int64', 'float64'])
df_no_outliers = df[(np.abs(zscore(df_numeric)) < 3).all(axis=1)]

print(f"Dataset size before removing outliers: {df.shape[0]}")
print(f"Dataset size after removing outliers: {df_no_outliers.shape[0]}")

import pandas as pd

# Attempt to load dataset safely
try:
    file_path = "student-dataset.csv"  # Ensure correct path
    df = pd.read_csv(file_path, sep=';')
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: File not found. Please check the file path.")
except pd.errors.ParserError:
    print("Error: Failed to parse CSV file. Check the delimiter and file format.")

# Fill missing values using appropriate methods
df.fillna(df.mean(numeric_only=True), inplace=True)  # Fill numeric columns with mean
df.fillna(df.mode().iloc[0], inplace=True)  # Fill categorical columns with mode

print("Missing values handled successfully!")

from scipy.stats import zscore
import numpy as np

# Remove outliers from numeric columns
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
df = df[(np.abs(zscore(df[numeric_columns])) < 3).all(axis=1)]

print("Outliers removed successfully!")

# Identify categorical columns
categorical_columns = ['sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian']

# Apply One-Hot Encoding safely
df = pd.get_dummies(df, columns=[col for col in categorical_columns if col in df.columns], drop_first=True)

print("Categorical columns converted using One-Hot Encoding!")

pip install streamlit pandas scikit-learn

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.exceptions import NotFittedError

@st.cache_data
def load_data():
    file_path = "student-dataset.csv"  # Ensure correct path
    try:
        df = pd.read_csv(file_path, sep=';')
        return df
    except FileNotFoundError:
        st.error("Error: File not found. Please check the file path.")
        st.stop()
    except pd.errors.ParserError:
        st.error("Error: Failed to parse CSV file. Check the delimiter and file format.")
        st.stop()

df = load_data()

# Fill missing values properly
df.fillna(df.mean(numeric_only=True), inplace=True)  # Numeric columns filled with mean
df.fillna(df.mode().iloc[0], inplace=True)  # Categorical columns filled with mode

st.write("Missing values handled successfully!")

# Define categorical columns including 'school'
categorical_columns = ['sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'school']

# Apply One-Hot Encoding safely
df = pd.get_dummies(df, columns=[col for col in categorical_columns if col in df.columns], drop_first=True)

st.write("Categorical columns converted using One-Hot Encoding!")

# Select numerical features for scaling, excluding target variable
numeric_features = df.select_dtypes(include=['number']).columns.drop("G3")

# Apply scaling to numerical features
scaler = StandardScaler()

try:
    df[numeric_features] = scaler.fit_transform(df[numeric_features])
except Exception as e:
    st.error(f"Error in scaling: {e}")
    st.stop()

# Define features & target
X = df.drop(columns=["G3"])
y = df["G3"]

# Split data safely
try:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    st.write("Data split into training and testing sets!")
except ValueError as e:
    st.error(f"Error in dataset splitting: {e}")
    st.stop()

# Train model safely
try:
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    st.write("Model training completed!")
except Exception as e:
    st.error(f"Model training error: {e}")
    st.stop()

st.title("AI-Powered Student Grade Prediction")

# User input
user_input = {}
for col in X.columns:
    # Check if the column is numeric before calling st.number_input
    if pd.api.types.is_numeric_dtype(X[col]):
        user_input[col] = st.number_input(
            f"Enter value for {col}",
            min_value=float(X[col].min()),
            max_value=float(X[col].max())
        )
    else:
        # Handle non-numeric columns (e.g., using st.selectbox)
        unique_values = X[col].unique().tolist()
        user_input[col] = st.selectbox(f"Select value for {col}", unique_values)

# Predict student grade
# ... (rest of the code remains the same) ...

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.exceptions import NotFittedError

def predict_student_grade(input_data, model, scaler, numeric_features):
    """
    Predicts final student grade based on input features.

    Args:
    - input_data (dict): Dictionary containing student attributes.
    - model (sklearn model): Trained machine learning model.
    - scaler (StandardScaler): Fitted scaler for preprocessing.
    - numeric_features (list): List of numerical feature names.

    Returns:
    - float: Predicted grade or error message.
    """
    try:
        # Convert user input dictionary into DataFrame
        input_df = pd.DataFrame([input_data])

        # Validate input: Check for missing columns
        missing_features = [col for col in numeric_features if col not in input_df.columns]
        if missing_features:
            return f"Error: Missing features in input: {missing_features}"

        # Ensure correct ordering of features
        input_df = input_df[numeric_features]

        # Handle potential missing values with mean imputation
        input_df.fillna(input_df.mean(), inplace=True)

        # Apply scaling with error handling
        try:
            input_df[numeric_features] = scaler.transform(input_df[numeric_features])
        except NotFittedError:
            return "Error: Scaler not fitted. Ensure preprocessing was completed."

        # Make prediction safely
        prediction = model.predict(input_df)[0]

        return round(prediction, 2)

    except Exception as e:
        return f"Prediction error: {e}"

# Example student input
student_data = {
    "studytime": 3,
    "absences": 2,
    "health": 4,
    "Dalc": 1,
    "Walc": 2
}

predicted_grade = predict_student_grade(student_data, model, scaler, numeric_features)
print(f"Predicted Final Grade (G3): {predicted_grade}")

pip install gradio

# Re-run this cell first
!pip install gradio

# After running the above cell, restart the Jupyter Notebook kernel.
# Then, run this cell again.
import gradio as gr
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor

# Load trained model (Ensure model is already trained in previous cells)
# You need to ensure 'model', 'scaler', and 'numeric_features' are defined
# in the cells preceding this Gradio app code. Assuming they are defined
# from the previous data loading, preprocessing, and training steps.

# Example definition of placeholder variables if you are testing this cell in isolation
# Replace this with the actual variables from your notebook
# model = RandomForestRegressor() # Ensure this is trained elsewhere
# scaler = StandardScaler() # Ensure this is fitted elsewhere
# numeric_features = ['studytime', 'absences', 'health', 'Dalc', 'Walc'] # Ensure this is defined elsewhere


def predict_student_grade(studytime, absences, health, Dalc, Walc):
    """
    Predicts final student grade (G3) based on input features.

    Args:
    - studytime (int): Study time per week.
    - absences (int): Number of absences.
    - health (int): Health rating.
    - Dalc (int): Daily alcohol consumption.
    - Walc (int): Weekend alcohol consumption.

    Returns:
    - float: Predicted grade.
    """
    # Ensure 'numeric_features' list includes all features used by the model
    # This function assumes the model is trained on exactly these 5 features,
    # and that the scaler was fitted on these features.
    input_data = pd.DataFrame([[studytime, absences, health, Dalc, Walc]],
                              columns=["studytime", "absences", "health", "Dalc", "Walc"])

    # Apply scaling
    # This line assumes 'scaler' was fitted on the same columns and in the same order.
    # If your original numeric features list was different or had a different order,
    # you might need to adjust this.
    try:
        input_data[numeric_features] = scaler.transform(input_data[numeric_features])
    except Exception as e:
         # Added error handling for scaling during inference
        print(f"Scaling error during prediction: {e}")
        # Return an error message or raise an exception if scaling fails
        return f"Error processing input: {e}"


    # Make prediction
    try:
        prediction = model.predict(input_data)[0]
        return round(prediction, 2)
    except Exception as e:
        # Added error handling for model prediction
        print(f"Prediction error: {e}")
        return f"Error making prediction: {e}"


# Initialize Gradio interface
# Ensure the inputs match the features expected by the predict_student_grade function
iface = gr.Interface(
    fn=predict_student_grade,
    inputs=[
        gr.Number(label="Study Time (hours/week)", value=3), # Added default values
        gr.Number(label="Absences", value=2),
        gr.Number(label="Health Rating (1-5)", value=4),
        gr.Number(label="Daily Alcohol Consumption (1-5)", value=1),
        gr.Number(label="Weekend Alcohol Consumption (1-5)", value=2)
    ],
    outputs="text",
    title="AI-Powered Student Grade Prediction",
    description="Enter student details to predict the final grade (G3)."
)

# Launch Gradio app
# Add share=False for local development unless you specifically need a public link
iface.launch(share=False)



import gradio as gr
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor

# Load trained model (Ensure model is already trained)
def predict_student_grade(studytime, absences, health, Dalc, Walc):
    """
    Predicts final student grade (G3) based on input features.

    Args:
    - studytime (int): Study time per week.
    - absences (int): Number of absences.
    - health (int): Health rating.
    - Dalc (int): Daily alcohol consumption.
    - Walc (int): Weekend alcohol consumption.

    Returns:
    - float: Predicted grade.
    """
    input_data = pd.DataFrame([[studytime, absences, health, Dalc, Walc]],
                              columns=["studytime", "absences", "health", "Dalc", "Walc"])

    # Apply scaling
    input_data[numeric_features] = scaler.transform(input_data[numeric_features])

    # Make prediction
    prediction = model.predict(input_data)[0]

    return round(prediction, 2)

# Initialize Gradio interface
iface = gr.Interface(
    fn=predict_student_grade,
    inputs=[
        gr.Number(label="Study Time (hours/week)"),
        gr.Number(label="Absences"),
        gr.Number(label="Health Rating (1-5)"),
        gr.Number(label="Daily Alcohol Consumption (1-5)"),
        gr.Number(label="Weekend Alcohol Consumption (1-5)")
    ],
    outputs="text",
    title="AI-Powered Student Grade Prediction",
    description="Enter student details to predict the final grade (G3)."
)

# Launch Gradio app
iface.launch()